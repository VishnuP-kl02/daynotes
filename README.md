Day 1

1.EdClub
2.Linux partition
3.Python - libraries
4.Python - Visual Studio Code
5.Lactex programming language
6.Version control - github - Tensorflow
7.Google Analystics
8.https://pages.github.com/
9.Portfolio template
10.Markdown format
11.StackEdit
12.HTML
13.Github - python


Create new folder in Github as another repository


Char --> ASCII code - 1 bit		AI, ML, Deep Learning, Computer Vision
Int - 8 bit
Binary - 1 bit




Unordered Lists: Use asterisks (*), plus signs (+), or hyphens (-) followed by a space.

Ordered Lists: Use numbers followed by a period.
1. First item
2. Second item
   1. Subitem 1



Day 2

Project  Mangement -  Jira

SDLC lifecycle

1.Requirement Gathering
2.Analysing the requirements  
3. Designing
4. Development phase
5.Testing
6.Deployement
7.Maintainence

AGILE methodology - Product Owner, (Dev and Quality Analysis) - Scrum, Scrum Master

P.O - Features of User in Agile - User Story. All user story combined and its gives Epic. If the product is delayed the product backlog. These are done by P.O. 

SM - covers backlog, covers features with Dev & QA, Assigns tasks. Name is given as Sprint for certain backlog of 2 weeks.

Zephyr and Power BI test manger for JIRA.

In JIRA we cannot add testcases, we need to use plugins like Zephyr to add testcases.
These are like plugins which are added upon.


Day 3

Neural Network 

1.Perceptron - ANN
Sum(F) = x1*w1+x2*w2+x3*w3+…..

There are several computing units which are acted upon. But whereas in GPU these are computed parallel, thus making computation faster.

In 2005 - image-net, data set of several images.
2.AGI
3.How AI and ML impact general lives especially on the field of autonomous driving.

Limitation - Data reading might not be correct as exactly analyzed.

Applications :- Travel, Heath-care, Sales, Marketing, Social Media, Automation, Credit & Insurance

Machine Learning consists of Supervised and Unsupervised learning.

Supervised has Classification and Regression. - It requires guidance and labels should be given.
Unsupervised has Clustering and Association. - It doesn’t guidance and no labels required.


Python:

a.Object oriented
b.High level
c.Interpreted

Libraries in Python

NumPy is  a python library for large multi dimensional array processing.
Pandas - for characters, strings etc
Scikit learn - machine learning library

DATA SCIENCE LIFE CYCLE

Problem Understanding ----> Data Collection ----> Data Cleaning & preparation ----> Exploratory Data Analysis ----> Feature Engineering ----> Machine Learning ----> Data Model Evaluation ----> Data Visualization



Day 4

Statistics

Mode, Mean, Median - Central Tendency			 Percentile

Descriptive Statistics : Descriptive statistics are tabular, graphical, and numerical summaries of data. The purpose of descriptive statistics is to facilitate the presentation and interpretation of data.  It include Tabular, Graphical and Numerical methods.

Distribution curve, Bell curve

Percentile : Upto a certain limit where it is used

Range - Spread if Distribution		R = Large - Small
Standard Deviation :  measure of how spread out numbers are.
  

When SD = 0 the graph of the distribution curve will be a straight line 

Variance = sq of Standard Deviation
. 

Qualifier

Q1 - 25th percentile					LB = Q1 - 1.5 * IQR
Q2 - 50th percentile (Median)
Q3 - 75th percentile					UB = Q3 - 1.5 * IQR

IQR - Inter Quartile Range = Q3 - Q1

Box plots are useful as they provide a visual summary of the data enabling researchers to quickly identify mean values, the dispersion of the data set, and signs of skewness.


Co-relation coefficient :- A correlation coefficient is a number between -1 and 1 that tells you the strength and direction of a relationship between variables.

Types - Pearson coefficient, Sphearman coefficient.

Histogram

Cumulative Distribution function (CDF)

While plotting we can represent as Probablity density function (pdf) and Cumulative density function (cdf)

ZScore :  z =  (x - mu)/sigma        mu- avg, sigma - SD 

Scatter Plot - Bivariate analysis

Confidence Interval : The confidence interval is the range of values that you expect your estimate to fall between a certain percentage of the time 
 For example, if you construct a confidence interval with a 95% confidence level, you are confident that 95 out of 100 times the estimate will fall between the upper and lower values specified by the confidence interval.



Day 5

Probability

Gaussian Distribution, Normal Distribution-- Special case of Gaussian

Normal Distribution = Zero mean
In Maths there are mostly reversible transformations only.

Sampling and Population

Central Limit Theorem - The central limit theorem states that the sampling distribution of a sample mean is approximately normal if the sample size is large enough, even if the population distribution is not normal.

Significance level

P value -  calculated as a difference between two probablities.
Paired test -- Null Hypotheis

Null Hypothesis & Alternate Hypothesis : 

Null hypothesis (H0): there is no difference in longevity between the two groups.
Alternative hypothesis (HA or H1): there is a difference in longevity between the two groups.

C-Statistic - ROC curve: 


The z-score for the sampling distribution of the sample means is

where μμ is the mean of the population the sample is taken from, σσ is the standard deviation of the population the sample is taken from, and nn is the sample size.


Statistic is a measure which describes a fraction of population.	Parameter refers to a measure which describes population.
Variable and Known	Fixed and Unknown
x̄ = Sample Mean,	μ = Population Mean
s = Sample Standard Deviation	σ = Population Standard Deviation
p̂ = Sample Proportion	P = Population Proportion
x = Data Elements	X = Data Elements
n = Size of sample	N = Size of Population
r = Correlation coefficient	ρ = Correlation coefficient


Probability sampling is a sampling technique, in which the subjects of the population get an equal opportunity to be selected as a representative sample. Its Random sampling

Non-probability sampling is a method of sampling wherein, it is not known that which individual from the population will be selected as a sample.Its non - Random sampling

Sampling error is one which occurs due to unrepresentativeness of the sample selected for observation.  occurs due to the sample selected does not perfectly represents the population of interest. Deviation between sample mean and population mean

Conversely, non-sampling error is an error arise from human error, such as error in problem identification, method or procedure used, etc. due to sources other than sampling, while conducting survey activities. Deficiency and analysis of data


Day 6, (day5) conti..

Statistics - Case Study

Uniform Probablity distribution & Guassian Probabllity distribution, Log- normal distrbution

Skewed symmetric curve ---------------> +ve skewed and -ve skewed

Guassian / Normal Distribution : is a type of continuous probability distribution that is symmetrical about its mean; most observations cluster around the mean, and the further away an observation is from the mean, the lower its probability of occurring. 


 Log- normal : - The log-normal distribution is a right skewed continuous probability distribution, meaning it has a long tail towards the right.

Covariance of two elements              


Bernoulli Distribution

A Probability Mass Function (PMF) describes the distribution of outcomes for a discrete probability function like the Bernoulli distribution. Typically, the outcomes are denoted as k = 1 for a success and k = 0 for a failure.
The PMF below describes the probability of each outcome in the Bernoulli distribution
oMean = p
oVariance = p(1 – p) = pq


Box- Cox function

It is a commonly used method for transforming a non-normally distributed dataset into a more normally distributed one. statistical technique that transforms your target variable so that your data closely resembles a normal distribution.

  

T-test
If the population variance is unknown or the sample size is small (n < 30), This test is more suitable for cases with limited data and unknown population variance, as it employs the Student’s t-distribution.
Condition 1: The size of your sample from the population is less than 30 observations.Condition 2: The population Standard Deviation is not known.
t = (x-μ)/ (s/√n), where x is the sample mean, μ is the population mean, s is the sample standard deviation and n is the sample size.
·  One-sample t-test: Compares the sample mean to a known value or population mean.
·  Two-sample t-test (Independent): Compares the means of two independent groups.
·  Paired t-test: Compares the means from the same group at different times (e.g., before and after treatment).

Formula (for a two-sample t-test): 
 				

Z-Test
If the population variance is known and the sample size is large (n > 30). The z-test relies on the standard normal distribution, making it appropriate for situations where the population variance is known and the sample size sufficiently large.
If the calculated Z-score is in the rejection region (outside the acceptance region), we reject the null hypothesis and when it is in the acceptance region, we fail to reject the null hypothesis.      Z = (x-μ)/ (σ/√n).   


P-Test
The P-value is not a test by itself but a measure used in hypothesis testing to help determine the significance of your results.

·  Low P-value (≤ 0.05): Reject the null hypothesis. There is strong evidence against the null hypothesis.
·  High P-value (> 0.05): Fail to reject the null hypothesis. There is weak evidence against the null hypothesis.

It tells you the probability of obtaining results as extreme as, or more extreme than, the results observed in your sample, under the assumption that the null hypothesis is true.

F-test 
Formula: For the F-test (comparing two variances):


est	Purpose	Assumptions	Sample Size	Use Case
P-value	Helps determine the significance of a test result.	Based on test type (t-test, z-test, etc.).	Any	Interpretation for hypothesis tests
T-test	Compares means of small samples (when population std dev is unknown).	Data is normally distributed.	Small (<30)	Mean comparison between groups
Z-test	Compares sample data to population data (when std dev is known).	Data is normal or n ≥ 30.	Large (≥30)	Population mean comparison
F-distribution	Compares variances or tests for differences between multiple group means.	Data is normal.	Any	Variance comparison, ANOVA


Day 7

Vectors, Matrices, 1D, 2D, 3D
Line : y = mx+c
Plane : ax+by+c = 0

Project of a vector a onto b 

a.b/(magn of b)     =    a.b/(mod b) =   mod a * cos(angle between them)
 
Broadcasting in Python :- It is the ability of NumPy (and other libraries like TensorFlow or PyTorch) to perform element-wise operations on arrays of different shapes and automatically expand (or "broadcast") the smaller array to match the shape of the larger array. 	Broadcasting allows you to perform operations on arrays with different dimensions and shapes without the need to explicitly replicate the data.
The basic idea is that NumPy will "stretch" or "broadcast" the smaller array along the dimensions of the larger array so that their shapes match.
Span of Vectors : The span of a given pair of vectors is the set of all possible linear combinations of those vectors. In simple terms, it’s the collection of all vectors that can be formed by scaling the given vectors and adding them together.
For two vectors v₁ and v₂ in a vector space, their span is the set of all vectors v that can be expressed as:
v= c1 * v1 + c2 * v2
where: c₁ and c₂ are scalars (real or complex numbers depending on the field of the vector space),v₁ and v₂ are the given vectors.
Activation functions can be non linear : 
·  Representation Power: Nonlinear activation functions allow the network to model complex relationships between inputs and outputs. This makes deep neural networks capable of representing highly complex functions that linear models cannot.
·  Backpropagation: Nonlinear functions provide useful gradients during backpropagation, which are necessary for updating weights in the training process.
·  Avoiding Linear Models: If all the activation functions in a neural network were linear, no matter how many layers the network had, it would still only represent a single linear transformation of the input. Nonlinearity makes it possible for neural networks to approximate arbitrary complex functions..

Below written are some examples of non- linear activation functions
1.  Sigmoid
Formula: f(x)=11+e−xf(x) = \frac{1}{1 + e^{-x}}f(x)=1+e−x1​
Range: (0, 1)
Use: Often used in binary classification problems, as its output can be interpreted as a probability.
Limitations: Can cause vanishing gradients during backpropagation, especially with very large or very small inputs.
2. Tanh (Hyperbolic Tangent)
Formula: f(x)=ex−e−xex+e−xf(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}f(x)=ex+e−xex−e−x​
Range: (-1, 1)
Use: Similar to sigmoid but outputs values centered around 0, which helps with gradient flow during training.
Limitations: Still suffers from vanishing gradients for large positive or negative inputs.
3. ReLU (Rectified Linear Unit)
Formula: f(x)=max⁡(0,x)f(x) = \max(0, x)f(x)=max(0,x)
Range: [0, ∞)
Use: The most commonly used activation function in modern neural networks, especially in deep networks. It is computationally efficient and helps mitigate the vanishing gradient problem.
Limitations: Can suffer from "dead neurons" when inputs are negative, leading to zero gradients (i.e., neurons that don't learn).
4. ELU (Exponential Linear Unit)
Formula: f(x)={xif x≥0α(ex−1)if x<0f(x) = \begin{cases} x & \text{if } x \geq 0 \\ \alpha (e^x - 1) & \text{if } x < 0 \end{cases}f(x)={xα(ex−1)​if x≥0if x<0​
Range: (-α, ∞)
Use: ELU is an improvement over ReLU, as it allows negative outputs for negative inputs, which can improve learning dynamics by providing non-zero gradients for negative values.
5. Soft-max
Formula: f(xi)=exi∑jexjf(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}f(xi​)=∑j​exj​exi​​
Range: (0, 1), outputs sum to 1 (probability distribution).
Use: Commonly used in the output layer of a neural network for multi-class classification problems. It converts raw scores into a probability distribution over classes.
Eigenvectors : Vectors that remain in same direction after a linear transformation. It does not change direction under transformation.
Eigenvalues : Scalars associated with eigenvectors. Represent the factor by which eigenvector is scaled during the transformation
AX=λX 				(A−λI)X=0   			det(λI−A)=0

UV Decomposition or Singular Value Decomposition, Dimensionality Reduction

Day 8

Probability 

Sample space : set of all possible outcomes
Event : anyone of sample space

P(Event) = favourable outcome / Total outcome

P(A∩B) = P(A) × P(B), 
where P(A∩B) = Probability of both independent events “A” and "B" happening together, P(A) = Probability of an event “A”, P(B) = Probability of an event “B”
To calculate P(A∩B) for dependent events, we use the concept of conditional probability and rewrite the formula as,
P(A∩B) = P(A|B)*P(B)  or  P(A∩B) = P(B|A)*P(A)

P(A|B) = P(B|A)*P(A) / (P(B))
	 = P(B|A)*P(A) / (P(B|A)*P(A) + P(B|~A)*P(~A))
	 = (P(B|A) * P(A)) / ((P(B|A)*P(A)) + (P(B|not A)*P(not A)))

Expected Value: If O represents an outcome of an experiment and n(O) represents the value of that outcome, then the expected value of the experiment is:
    ∑n(O)×P(O)∑n(O)×P(O)
where Σ is the “sum,” meaning we add up the results of the formula that follows over all possible outcomes.

Entropy : entropy gives a measure of the uncertainty of the random variable. It is sometimes called the missing information: the larger the entropy, the less a priori information one has on the value of the random variable.
  

H(P) = - ∫ P(x) * log₂(P(x)) dx

H(P) = - ∑ P(x) * log₂(P(x))

Entropy plays a crucial role in information theory, as it serves as the foundation for optimal encoding and data compression techniques like Huffman coding or arithmetic coding.
	
Binomial Distribution : P(X = r) = nCk * p^k * q^(n-k), r = 0, 1, 2, 3….

p is success, q is failure and q = 1 – p, p, q > 0 such that p + q = 1

P(x:n,p) = n!/[x!(n-x)!].p^x.(q)^n-x

Mean, μ = np, Variance, σ2 = npq, Standard Deviation σ= √(npq)

 The multiplication law is potentially helpful when we are interested in computing the probability of The intersection of two events
 Bayes’s Theorem is expressed as: Bayes’s Theorem to calculate conditional probabilities, specifically allowing them to update the probability of a hypothesis based on new evidence or data.
P(H∣E)=P(E∣H)⋅P(H)P(E)P(H \mid E) = \frac{P(E \mid H) \cdot P(H)}{P(E)}P(H∣E)=P(E)P(E∣H)⋅P(H)​

Day 9

Derivative
Cost Function


Mean Absolute Error

i = index of sample
ŷ = predicted value
y = expected value
m = number of samples in the data set

The goal of any Machine Learning model is to minimize the Cost Function. 
One common function that is often used is the mean squared error, which measures the difference between the actual value of y and the estimated value of y (the prediction). 


Chain Rule : dy/dx = (dy/du)*(dudx)


Day 10

VS Code , HTML, CSS, JavaScript, Typescript, JSON, XML, Markdown

YAML, TOML, INI, CSV, SQL

File is saved in .py extension. It is extensible and Integrated terminal
It has Extensible Arch

PyCharm application - Database and Scientific tools, Smart Code assistance, 
Python-centric. It contains all the required materials in that IDE.

Whereas in other types like VS Code, we need to install the necessary contents

Jupiter Notebook and Google Collab : Data exploration ,visualization and sharing.
It contains Free GPU/TPU support for ML. It can combine code, text and visualizations. It requires internet. Its beginner friendly application.

Spyder :  Scientific Computing

Anacoda : All packages included, Cross platform for all.. Includes all data science libraries (NumPy, Pandas, Matpolib).


Day 11

Python - Numeric, String, Sequence, List, Tuple, Dictionary, Set and Boolean
Python relies on indentation, using white space, to define scope; such as the scope of loops, functions and classes. Other programming languages often use curly-brackets for this purpose.

1.If we put a number it will give the same or any # comments it just portrait as a comments to be added upon.
a)List
b)Tuple
c)Range

Mapping type :
1.Dictionary
2.Set
3.None type

Operators
i.Arithmetic
ii.Comparison
iii.Logical
iv.Membership
v.Identity


Day 12


Multiplication table 1 to 10

for i in range(1, 11):
    print(f"multiplication table of {i}:")
    for j in range(1, 11):
        print(f"{i} x {j} = {i * j}")
print()

Or 

for i in range(1, 11):
    table = [f"{i} x {j} = {i * j}" for j in range(1, 11)]
    print(" , ".join(table))


Day 13

Numpy : - Numerical Python

Used in multi-dimensional arrays and for Scientific computing. Applications in deep and machine learning.
Also has various libraries like Pandas, Matpolib and Scikit-learn.

High performance and efficient storage and data operations as arrays grow in size.
N - dimensional arrays and store elements of same type and size. 
 
1 D - Vector ;  2 D - Matrix
NumPy arrays have only homogenous element.

Pandas:-

Advantages:
Fast, efficient. Size mutability, Easy handling of missing data, Data fromk different objects can be loaded.

Python package that provides fast, flexible, and expressive data structures designed to make working with "relational" or "labeled" data both easy and intuitive. 
It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. 

1.Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving/loading data from the ultrafast HDF5 format
2.Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging.
3.Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations
4.Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
5.Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects.


Day 15  SQL Zoo.net

SQL - Structured Query Language

Databases: Simple, Easier and Reliable to use. Eg of database is Oracle, MyQL
I.Databases backup the data at multiple places to increase reliability
II.Its done without much programming background
III.Technique called indexing is used in SQL for Databases

Relational databases: stores data in structured tables with rows and columns using preferred schema. They are ideal for highly structured data with complex queries. 
Eg SQL

Non Relational databases : known as non- SQL offers more flexibility by storing data in various formats like documents, key-pair values or graphs allowing less structured-and more dynamic data sets.Handle large volumes of diverse data.
Eg MongoDB

Tables
Relational database are stored across multiple tables

Table 1: movie_id, movie_name, year
Table 2: actor_id, actor_name, gender
Table 3:  movie_id, actor_id
Here movie_id and actor_id are unique. There is primary key associatyed with each tables

SQL is not general purpose programming language it sdomain specific. It has standard way to query/delete/add/modify data. 

General purpose programming language lets us to create web/mobile applications, DBs, Operating Systems

C,C++, python are procedural programming  language whereas SQL is declarative programming language

Execution of SQL : SQL->Parser, compiler
		              ->parser: tries to understand query
			 ->compiler: query optimizer and generates code
			 ->query executer -> DB -> results

IMDB dataset
a)Amazon.com
b)Tables 
c)All categories of Directors, actors, movie_genre, roles, director_genre

Logical Operators : AND, OR, ALL, NOT, BETWEEN, EXIST, IN, LIKE, SOME
Day 16

Absolute path and Relative path

Relative path is recommended.
File Path operations - Join path, Split path, File extension

https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html - conda envireonment


Day 16

Absolute path and Relative path

Relative path is recommended.
File Path operations - Join path, Split path, File extension

https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html - conda envireonment
 

Day 17

Visualization 
1.Matpolib
2.Figures
3.Axis-labels
4.Sub-figures
5.Ticks

Types of Plots
1)Line Plot
2)Bar chart
3)Pie chart
4)Histogram
5)Cdf plot
6)Kde plot
7)Scalar plot
8)Heatmaps
9)Box plots
10)Scatter plot - with colour


Feature Engineering

1.Feature Orthogonality
a)Cosine Similarity
2.Feature Colinerity
3.Feature Slicing
4.Indicator Variable
5.Feature Binning
6.Mathematical transforms	
a)Logarithms
b)FFT & STFT
Day 18

Tableau / Power BI - Business intelligence tool

Explorative analysis, Descriptive analysis

Tableau is a data visualization and business intelligence tool that enables users to connect, visualize and share data in a highly interactive and intuitive way. It allows users to quickly analyze and explore large and complex datasets using a drag-and-drop interface without requiring coding or programming skills. Tableau provides a wide range of chart types and visualization options, such as line charts, bar charts, maps, scatter plots, and many more.

Tableau can connect to various data sources, including databases, spreadsheets, big data platforms, and cloud services. It also allows users to perform data cleaning and transformation tasks, create custom calculations, and generate insights using advanced analytics features.


Features of Tableau
Tableau is a powerful data visualization and business intelligence tool that offers a wide range of features to help users analyze, visualize, and share data. Some of the critical features of Tableau include the following:

Data Visualization: Tableau offers a variety of chart types, including bar charts, line charts, scatter plots, maps, and many more. It also provides interactive dashboards and visualizations that allow users to explore data and gain insights quickly.
Data Exploration: Tableau allows users to explore data in depth by drilling into data points, filtering data, and creating hierarchies.
Data Modeling: Tableau enables users to create and manage relationships between different data sources, define calculations, and create measures and KPIs.
Data Preparation: Tableau includes a suite of data preparation tools that allow users to reshape and clean data for analysis.
Collaboration: Tableau allows users to share and collaborate on reports and dashboards with colleagues and integrate with other tools like Slack, Salesforce, and Google Drive.
Mobile Access: Tableau provides a mobile app that allows users to access reports and dashboards from their mobile devices.
Natural Language Processing: Tableau includes natural language processing capabilities that allow users to ask questions in natural language and receive answers in the form of visualizations.
Real-time data: Tableau can connect to real-time data sources like Amazon Kinesis, Apache Kafka, and Tableau Server extracts and display real-time data in visualizations.
